{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9051fa06",
   "metadata": {},
   "source": [
    "# Multilingual Word Embedding using trained GOOD mappings\n",
    "\n",
    "- This project based on [UMWE](https://github.com/ccsasuke/umwe).\n",
    "- The purpose of this project is change the algorithm of UMWE \n",
    "    - to use GOOD mapping which have good performance Cross-lingual Word Embedding(CWE), such as between english and franch, \n",
    "    - and to increase the performance of CWE to other language.(such as Japanese)\n",
    "    \n",
    "- **Explain the detail**\n",
    "  - detail 1\n",
    "  - detail 2\n",
    "  \n",
    "\n",
    "- EN / FR / ES and JA / ZN / PL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9dec5",
   "metadata": {},
   "source": [
    "## 1. Create dictionaries for evaluating\n",
    "\n",
    "- We could evaluating dictionaries from [here](https://github.com/facebookresearch/MUSE#ground-truth-bilingual-dictionaries), that is dataset of MUSE project.\n",
    "- But most of the dataset for evaluating BLI performance about English, (but I'm also interesting the CWE into Japanese)\n",
    "- So, we need to create some dictionaries using EN-XX and XX-EN.\n",
    "    - XX-EN=EN-JA : XX-JA\n",
    "    - JA-EN=EN-XX : JA-XX\n",
    "- I consider the using, EN / FR / ES as the Source Languages and JA / ZN / PL as the Target Languages, \n",
    "    - So, I need to create Source-Target and Target-Source for all pairs for evaluating.\n",
    "    - **support_han/1.Create_dictionaries.ipynb**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90deacec",
   "metadata": {},
   "source": [
    "## 2. Create GOOD mappings\n",
    "\n",
    "- This project use GOOD mappings trained from EN, FR and ES.\n",
    "- So, Let's train the GOOD mappings using,\n",
    "    - Superviesed : Use 5000 pairs information from dictinoary\n",
    "    - **1.Train_sup.ipynb**\n",
    "    - Unsupervised : Use GAN for aligning the word embedding continuous distributions.\n",
    "    - **2.Train_unsup.ipynb**\n",
    "- I want to check what the difference between training types will affect to the results\n",
    "\n",
    "- more detail ~\n",
    "    - Excel file : The relationship between supervised, unsupervised and Num of seed dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb711956",
   "metadata": {},
   "source": [
    "## 3. Train Multilingual Word Embedding for Target Language\n",
    "\n",
    "- I found the pivot mapping, which is using an another CWE when train 2 languages as a pivot, could make new dictionary seeds the original mapping could not predict.\n",
    "- One trained GOOD mapping can help three CWE training, for example\n",
    "    - SEED(EN - JA)        ::        SEED(EN - JA) + SEED(EN - FR - JA)\n",
    "    - SEED(FR - JA)        ::        SEED(FR - JA) + SEED(FR - EN - JA)\n",
    "    - SEED(EN - FR)        ::        SEED(good(EN - FR)) OR SEED(FR - JA) + SEED(FR - EN - JA) + SEED(EN - JA) + SEED(EN - FR - JA)\n",
    "- I hope the new dictionary seeds for tuning MWE.\n",
    "\n",
    "\n",
    "- **Detail description for Model**\n",
    "    - detail\n",
    "    - detail\n",
    "\n",
    "\n",
    "- 3.Train_sup_han.ipynb\n",
    "    - In this case, actually the performances of MWE are not that changed. \n",
    "    - dataset : Source Languages(EN, FR, ES) and Target Languages(JA, ZH, PL)\n",
    "    - GOOD mappings : EN-FR, FR-EN, EN-ES, ES-EN trainend supervised CWE\n",
    "    - In the case of supervised training, the **first mapping result is alway going to be the best one.**\n",
    "    - I could check \n",
    "        - the # of seeds are increased over 20% for every iteration,\n",
    "        - but because of the reason above, best models are from first training and **GOOD mapping does not seems to have any effect.**\n",
    "\n",
    "- 4.Train_unsup_han.ipynb\n",
    "    - dataset : Source Languages(EN, FR, ES) and Target Languages(JA, ZH, PL)\n",
    "    - GOOD mappings : EN-FR, FR-EN, EN-ES, ES-EN trainend unsupervised CWE\n",
    "    - In this case, the results are different depending on the langauages.\n",
    "        - (EN-FR) : JA , unsup_han < unsup\n",
    "        - (EN-FR) : ZH , unsup_han > unsup\n",
    "        - (EN-FR) : PL , unsup_han > unsup\n",
    "    - But, the results are worse than supervised MWE.\n",
    "    - I should check,\n",
    "        - What kinds of language pairs have good performance or bad performance.\n",
    "        - Is GOOD mapping seed have available effect to performance.\n",
    "    - So, I need to check\n",
    "        - Train with the language pairs introduced UMWE paper. (Pivot pairs and Direct pairs)\n",
    "        - Use the fasttext (or normalized fasttext) word embeddings.\n",
    "        - Make a model using the Multilingual Adversarial Training(MAT) pre-trained mapping to compare the effects of the using GOOD mapping.\n",
    "        - Test and analyze more detaily, by using BLI, sentiment analysis and classification test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15326fcc",
   "metadata": {},
   "source": [
    "## 4. Analysis for detail\n",
    "- **5.Train_with_pre_trained_mat.ipynb**\n",
    "    - wiki_unsupervised_enes_pl : There are some cases the training between EN and PL does not proceed.(MAT)\n",
    "    - So, the performance of MPSR is going bad. (EN-PL:0.2% and ES-PL:42%)\n",
    "    - BUT, when I use the UMWE with trained GOOD mapping, the bad mapping recovers the performance using GOOD mapping SEED.\n",
    "    - (EN-PL:58% and ES-PL:42%)\n",
    "    \n",
    "- MAT with 50000 words and MPSR with 200000 words\n",
    "    - 비슷비슷한듯/?\n",
    "\n",
    "- Empty embedding in target\n",
    "    - 실패\n",
    "\n",
    "- Japan with wikipedia2ved\n",
    "    - (es,it)-jp\n",
    "    - \n",
    "- low performance fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935060a",
   "metadata": {},
   "source": [
    "## Idea\n",
    "- Train MAT with small number and Train MPSR with big number\n",
    "\n",
    "- (it,fr)EN\n",
    "- (es,fr)EN\n",
    "- (es,it)EN\n",
    "    - ES - IT has GOOD mapping\n",
    "    - !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "- (en,fr)ALL\n",
    "\n",
    "- (en,es)IT   en-es, es-en !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "- de-it pivot>direct\n",
    "    - (en,de)IT   en-de, en-de !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "- it-de pivot>direct\n",
    "    - **(en-it)DE**\n",
    "    \n",
    "\n",
    "\n",
    "- es-fr pivot< direct\n",
    "    - (en,es)FR   en-es, es-en ???????????????????????????\n",
    "- it-es pivot< direct\n",
    "    - (en,it)ES   en-it, it-en !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "- fr-it pivot< direct\n",
    "    - (en,fr)IT ??????????????????????????\n",
    "\n",
    "\n",
    "                 \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafc17b",
   "metadata": {},
   "source": [
    "# Ref.\n",
    "- [Improving Cross-Lingual Word Embeddings by Meeting in the Middle](https://www.aclweb.org/anthology/D18-1027/)\n",
    "- [How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions](https://www.aclweb.org/anthology/P19-1070/)\n",
    "- [UNSUPERVISED HYPERALIGNMENT FOR MULTILINGUAL WORD EMBEDDINGS](https://openreview.net/pdf?id=HJe62s09tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae9ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (han)",
   "language": "python",
   "name": "han"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
